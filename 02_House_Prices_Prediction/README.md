# ğŸ  Kaggle å®æˆ˜ï¼šåŸºäº XGBoost çš„æˆ¿ä»·é¢„æµ‹å…¨æµç¨‹è§£æ

> **é¡¹ç›®èƒŒæ™¯**ï¼šåŸºäº Kaggle ç»å…¸çš„ [House Prices: Advanced Regression Techniques](https://www.kaggle.com/c/house-prices-advanced-regression-techniques) ç«èµ›æ•°æ®ã€‚
> **æ ¸å¿ƒæŠ€æœ¯**ï¼š`Pandas` æ¸…æ´—ã€`Log` å¹³æ»‘å˜æ¢ã€`XGBoost` å›å½’ã€ç‰¹å¾é‡è¦æ€§åˆ†æã€‚

## ğŸ“‹ ç›®å½•
1. [ç¯å¢ƒå‡†å¤‡](#step-1-ç¯å¢ƒå‡†å¤‡)
2. [ç›®æ ‡å€¼åˆ†æ](#step-2-ç›®æ ‡å€¼åˆ†æ)
3. [ç¼ºå¤±å€¼å¤„ç†](#step-3-ç¼ºå¤±å€¼å¤„ç†)
4. [ç‰¹å¾å·¥ç¨‹](#step-4-ç‰¹å¾å·¥ç¨‹)
5. [æ¨¡å‹è®­ç»ƒä¸è§£é‡Š](#step-5-æ¨¡å‹è®­ç»ƒ)
6. [ç»“æœæäº¤](#step-6-é¢„æµ‹ä¸æäº¤)

---

## Step 1: ç¯å¢ƒå‡†å¤‡

è¯»å–æ•°æ®å¹¶åˆ†ç¦» ID åˆ—ã€‚

```python
import pandas as pd
import numpy as np
import seaborn as sns
import xgboost as xgb

train_df = pd.read_csv("./data/train.csv")
test_df = pd.read_csv("./data/test.csv")

# å¤‡ä»½å¹¶åˆ é™¤ ID
train_ID = train_df['Id']
test_ID = test_df['Id']
train_df.drop("Id", axis=1, inplace=True)
test_df.drop("Id", axis=1, inplace=True)
# Log å˜æ¢
train_df["SalePrice"] = np.log1p(train_df["SalePrice"])
# One-Hot Encoding
all_data = pd.get_dummies(all_data)
model_xgb = xgb.XGBRegressor(learning_rate=0.05, n_estimators=2200)
model_xgb.fit(X_train, y_train)
log_predictions = model_xgb.predict(X_test)
final_predictions = np.expm1(log_predictions)
# ç”Ÿæˆ CSV...
