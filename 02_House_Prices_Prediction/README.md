# House Prices Prediction: Advanced Regression with XGBoost ğŸ 

> **Kaggle Competition:** [House Prices: Advanced Regression Techniques](https://www.kaggle.com/c/house-prices-advanced-regression-techniques)
> **Score:** Top 15% (Estimated)
> **Model:** XGBoost Regressor
> **Strategy:** Log-Transformation + Stratified Imputation

## 1. Project Overview (é¡¹ç›®ç®€ä»‹)
æœ¬é¡¹ç›®åŸºäº Kaggle ç»å…¸çš„ç»“æ„åŒ–æ•°æ®ç«èµ›ã€‚ä»»åŠ¡æ˜¯æ ¹æ®çˆ±è·åå·åŸƒå§†æ–¯å¸‚ä½å®…çš„ 79 ä¸ªç‰¹å¾ï¼ˆå¦‚åœ°åŸºã€è½¦åº“ã€åœ°ä¸‹å®¤ç­‰ï¼‰ï¼Œé¢„æµ‹æˆ¿å±‹çš„æœ€ç»ˆå”®ä»·ã€‚

* **éš¾ç‚¹ (Challenges)ï¼š**
    * **ç‰¹å¾ç¹å¤šï¼š** åŒ…å« 80 å¤šä¸ªç‰¹å¾ï¼Œä¸”æ··åˆäº†æ•°å€¼å‹å’Œç±»åˆ«å‹æ•°æ®ã€‚
    * **æ•°æ®åæ–œï¼š** ç›®æ ‡å€¼ï¼ˆæˆ¿ä»·ï¼‰å‘ˆç°ä¸¥é‡çš„å³ååˆ†å¸ƒï¼Œä¸ç¬¦åˆæ­£æ€å‡è®¾ã€‚
    * **ç¼ºå¤±å€¼å¤æ‚ï¼š** è®¸å¤š NaN å¹¶éæ•°æ®ä¸¢å¤±ï¼Œè€Œæ˜¯ä»£è¡¨â€œæ²¡æœ‰è¯¥è®¾æ–½â€ï¼ˆå¦‚æ²¡æœ‰æ³³æ± ï¼‰ï¼Œéœ€è¦ç‰¹æ®Šå¤„ç†ã€‚
* **æˆ‘çš„æ–¹æ¡ˆ (My Approach)ï¼š**
    * å¯¹ç›®æ ‡å€¼è¿›è¡Œ **Log1p å¹³æ»‘å¤„ç†**ï¼Œæ¶ˆé™¤ååº¦ã€‚
    * åˆ¶å®š **åˆ†å±‚ç¼ºå¤±å€¼å¡«å……ç­–ç•¥**ï¼ˆåŒºåˆ†â€œç‰©ç†ç¼ºå¤±â€ä¸â€œæ•°å€¼ç¼ºå¤±â€ï¼‰ã€‚
    * ä½¿ç”¨ **XGBoost** æ¢¯åº¦æå‡æ ‘æ¨¡å‹ï¼Œåˆ©ç”¨å…¶å¯¹éçº¿æ€§å…³ç³»çš„å¼ºå¤§æ‹Ÿåˆèƒ½åŠ›ã€‚

## 2. Tech Stack (æŠ€æœ¯æ ˆ)
* **Python 3.10**
* **Pandas & NumPy** (Data Manipulation)
* **Seaborn & Matplotlib** (EDA & Visualization)
* **XGBoost** (Gradient Boosting Framework)
* **Scikit-Learn** (Preprocessing)

---

## 3. Implementation Details (æ ¸å¿ƒå®ç°)

### 3.1 Data Loading & Log Transformation
ä¸ºäº†æå‡å›å½’æ¨¡å‹çš„æ”¶æ•›é€Ÿåº¦å’Œç²¾åº¦ï¼Œæˆ‘é¦–å…ˆæ£€æŸ¥äº†æˆ¿ä»·çš„åˆ†å¸ƒã€‚å‘ç°å…¶æ˜¾è‘—å³åï¼Œå› æ­¤é‡‡ç”¨ log(1+x) è¿›è¡Œå¹³æ»‘å¤„ç†ã€‚

![Target Distribution](images/target_dist.png)

```python
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
import xgboost as xgb

sns.set_style("whitegrid")

# 1. è¯»å–æ•°æ®
train_df = pd.read_csv("./data/train.csv")
test_df = pd.read_csv("./data/test.csv")

# 2. å¤‡ä»½å¹¶ç§»é™¤ ID (ä¸å‚ä¸è®­ç»ƒ)
train_ID = train_df['Id']
test_ID = test_df['Id']
train_df.drop("Id", axis=1, inplace=True)
test_df.drop("Id", axis=1, inplace=True)

# 3. ç›®æ ‡å€¼ Log å¹³æ»‘
# åŸå§‹æ•°æ®å³åä¸¥é‡ï¼ŒLog å˜æ¢ä½¿å…¶è¿‘ä¼¼æ­£æ€åˆ†å¸ƒ
train_df["SalePrice"] = np.log1p(train_df["SalePrice"])
print("âœ… ç¯å¢ƒå‡†å¤‡å®Œæˆï¼Œç›®æ ‡å€¼å·²è¿›è¡Œ Log å˜æ¢ã€‚")
```

### 3.2 Strategic Imputation (åˆ†å±‚ç¼ºå¤±å€¼å¤„ç†)
è¿™æ˜¯ç‰¹å¾å·¥ç¨‹ä¸­æœ€å…³é”®çš„ä¸€æ­¥ã€‚æˆ‘æ²¡æœ‰ç®€å•åœ°å…¨éƒ¨å¡«å……å‡å€¼ï¼Œè€Œæ˜¯æ ¹æ®ç‰¹å¾çš„**ç‰©ç†å«ä¹‰**å°†å…¶åˆ†ä¸ºä¸‰ç±»å¤„ç†ï¼š

* **ç‰©ç†ç¼ºå¤± (Fill "None"):** å¦‚ `PoolQC` ä¸ºç©ºï¼Œä»£è¡¨â€œæ²¡æœ‰æ¸¸æ³³æ± â€ã€‚
* **æ•°å€¼ç¼ºå¤± (Fill 0):** å¦‚ `GarageArea` ä¸ºç©ºï¼Œä»£è¡¨â€œè½¦åº“é¢ç§¯ä¸º0â€ã€‚
* **é‚»åŸŸç¼ºå¤± (Fill Median):** `LotFrontage`ï¼ˆè¡—é“è¿æ¥è·ç¦»ï¼‰é€šå¸¸ä¸åŒä¸€è¡—åŒºçš„é‚»å±…ç›¸ä¼¼ã€‚

![Missing Values](images/missing_values.png)

```python
# åˆå¹¶æ•°æ®ä»¥ä¾¿ç»Ÿä¸€å¤„ç†
ntrain = train_df.shape[0]
y_train = train_df.SalePrice.values
all_data = pd.concat((train_df.drop(["SalePrice"], axis=1), test_df)).reset_index(drop=True)

# --- ç­–ç•¥ A: ç‰©ç†æ„ä¹‰ä¸Šçš„"æ— " (å¡« "None") ---
cols_fill_none = ["PoolQC", "MiscFeature", "Alley", "Fence", "FireplaceQu",
                  "GarageType", "GarageFinish", "GarageQual", "GarageCond",
                  "BsmtQual", "BsmtCond", "BsmtExposure", "BsmtFinType1", "BsmtFinType2",
                  "MasVnrType"]
for col in cols_fill_none:
    all_data[col] = all_data[col].fillna("None")

# --- ç­–ç•¥ B: æ•°å€¼æ„ä¹‰ä¸Šçš„"0" (å¡« 0) ---
cols_fill_zero = ["GarageYrBlt", "GarageArea", "GarageCars",
                  "BsmtFinSF1", "BsmtFinSF2", "BsmtUnfSF","TotalBsmtSF",
                  "BsmtFullBath", "BsmtHalfBath", "MasVnrArea"]
for col in cols_fill_zero:
    all_data[col] = all_data[col].fillna(0)

# --- ç­–ç•¥ C: é‚»å±…çš„ä¸­ä½æ•°å¡«å…… ---
all_data["LotFrontage"] = all_data.groupby("Neighborhood")["LotFrontage"].transform(
    lambda x: x.fillna(x.median()))

# --- ç­–ç•¥ D: å‰©ä½™å°‘é‡çš„ä¼—æ•°å¡«å…… ---
cols_mode = ["MSZoning", "Electrical", "KitchenQual", "Exterior1st", "Exterior2nd", "SaleType", "Functional", "Utilities"]
for col in cols_mode:
    all_data[col] = all_data[col].fillna(all_data[col].mode()[0])

print(f"âœ… ç¼ºå¤±å€¼å¤„ç†å®Œæˆï¼å‰©ä½™ NaN: {all_data.isnull().sum().sum()}")
```

### 3.3 Feature Correlation & Model Training
åœ¨è¿›è¡Œ One-Hot ç¼–ç åï¼Œæˆ‘é€‰æ‹©äº† **XGBoost**ã€‚ç›¸æ¯”ä¼ ç»Ÿçš„çº¿æ€§å›å½’ï¼ŒXGBoost èƒ½è‡ªåŠ¨å¤„ç†ç‰¹å¾é—´çš„éçº¿æ€§äº¤äº’ï¼Œä¸”å¯¹å¼‚å¸¸å€¼æœ‰æ›´å¥½çš„é²æ£’æ€§ã€‚

é€šè¿‡ç‰¹å¾é‡è¦æ€§å›¾è¡¨ï¼Œæˆ‘ä»¬å¯ä»¥çœ‹åˆ° **OverallQual (æ•´ä½“è´¨é‡)** å’Œ **GrLivArea (å±…ä½é¢ç§¯)** æ˜¯å½±å“æˆ¿ä»·çš„æœ€æ ¸å¿ƒå› ç´ ã€‚

![Feature Importance](images/feature_importance.png)

```python
# 1. ç‰¹å¾ç¼–ç  (One-Hot)
all_data = pd.get_dummies(all_data)
X_train = all_data[:ntrain]
X_test = all_data[ntrain:]

# 2. æ„å»º XGBoost æ¨¡å‹
# å‚æ•°ç»è¿‡å¾®è°ƒä»¥å¹³è¡¡æ‹Ÿåˆèƒ½åŠ›ä¸æ³›åŒ–èƒ½åŠ›
model_xgb = xgb.XGBRegressor(
    colsample_bytree=0.46, gamma=0.04, 
    learning_rate=0.05, max_depth=3, 
    min_child_weight=1.5, n_estimators=2200,
    reg_alpha=0.46, reg_lambda=0.85,
    subsample=0.52, random_state=7, n_jobs=-1
)

# 3. è®­ç»ƒ
print("ğŸš€ å¼€å§‹è®­ç»ƒ XGBoost...")
model_xgb.fit(X_train, y_train)
print("ğŸ‰ è®­ç»ƒå®Œæˆï¼")

# 4. é¢„æµ‹ä¸è¿˜åŸ
# æ³¨æ„ï¼šé¢„æµ‹å‡ºçš„æ˜¯ Log å€¼ï¼Œéœ€è¦ç”¨ expm1 è¿˜åŸ
log_predictions = model_xgb.predict(X_test)
final_predictions = np.expm1(log_predictions)

# 5. ç”Ÿæˆæäº¤æ–‡ä»¶
submission = pd.DataFrame()
submission['Id'] = test_ID
submission['SalePrice'] = final_predictions
submission.to_csv('submission_eda_xgboost.csv', index=False)
print("âœ… ç»“æœæ–‡ä»¶å·²ç”Ÿæˆ: submission_eda_xgboost.csv")
```
