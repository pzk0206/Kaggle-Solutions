# ğŸ  Kaggle å®æˆ˜ï¼šåŸºäº XGBoost çš„æˆ¿ä»·é¢„æµ‹ (å®Œæ•´æµç¨‹)

> **é¡¹ç›®èƒŒæ™¯**ï¼š[House Prices: Advanced Regression Techniques](https://www.kaggle.com/c/house-prices-advanced-regression-techniques)
> **æ ¸å¿ƒç­–ç•¥**ï¼šç»†è‡´çš„ EDAã€åˆ†å±‚ç¼ºå¤±å€¼å¡«å……ã€Log ç›®æ ‡å€¼å¹³æ»‘ã€XGBoost å›å½’ã€‚

## ğŸ“‹ ç›®å½•
1. [Step 1: ç¯å¢ƒä¸æ•°æ®è¯»å–](#step-1)
2. [Step 2-3: ç›®æ ‡å€¼åˆ†æä¸ Log å˜æ¢](#step-2)
3. [Step 4-5: ç¼ºå¤±å€¼å¤„ç† (æ ¸å¿ƒç­–ç•¥)](#step-4)
4. [Step 6: ç‰¹å¾ç›¸å…³æ€§åˆ†æ](#step-6)
5. [Step 7-9: ç¼–ç ã€è®­ç»ƒä¸æäº¤](#step-7)

---

## Step 1: ç¯å¢ƒä¸æ•°æ®è¯»å– <a name="step-1"></a>

å¯¼å…¥å¿…è¦çš„åº“ï¼Œè¯»å–æ•°æ®ï¼Œå¹¶æå‰å‰¥ç¦» ID åˆ—ã€‚

```python
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
import xgboost as xgb

# è®¾ç½®ç»˜å›¾é£æ ¼
sns.set_style("whitegrid")

# è¯»å–æ•°æ®
train_df = pd.read_csv("./data/train.csv")
test_df = pd.read_csv("./data/test.csv")

# å¤‡ä»½ ID ç”¨äºæäº¤ï¼Œåˆ é™¤è®­ç»ƒæ•°æ®ä¸­çš„ ID
train_ID = train_df['Id']
test_ID = test_df['Id']
train_df.drop("Id", axis=1, inplace=True)
test_df.drop("Id", axis=1, inplace=True)

print(f"âœ… æ•°æ®è¯»å–å®Œæˆï¼è®­ç»ƒé›†å½¢çŠ¶: {train_df.shape}")
```

---

## Step 2-3: ç›®æ ‡å€¼åˆ†æä¸ Log å˜æ¢ <a name="step-2"></a>

åŸå§‹æˆ¿ä»·å‘ˆç°**å³ååˆ†å¸ƒ**ã€‚ä¸ºäº†è®©æ¨¡å‹æ›´å¥½åœ°è®­ç»ƒï¼Œæˆ‘ä»¬å¯¹ `SalePrice` æ‰§è¡Œ `log1p` å¹³æ»‘å¤„ç†ã€‚

![Target Distribution](images/target_dist.png)

```python
# --- å¯è§†åŒ–å¯¹æ¯” (Step 2) ---
plt.figure(figsize=(12, 5))
plt.subplot(1, 2, 1)
sns.histplot(train_df['SalePrice'], kde=True, color='blue')
plt.title('Original SalePrice Distribution')

plt.subplot(1, 2, 2)
sns.histplot(np.log1p(train_df['SalePrice']), kde=True, color='green')
plt.title('Log-Transformed Distribution')
plt.show()

# --- æ‰§è¡Œ Log å˜æ¢ (Step 3) ---
train_df["SalePrice"] = np.log1p(train_df["SalePrice"])
print("âœ… ç›®æ ‡å€¼ SalePrice å·²å®Œæˆ Log å¹³æ»‘å¤„ç†ã€‚")
```

---

## Step 4-5: ç¼ºå¤±å€¼å¤„ç† (æ ¸å¿ƒç­–ç•¥) <a name="step-4"></a>

è¿™æ˜¯æå‡åˆ†æ•°çš„å…³é”®ã€‚æˆ‘ä»¬ä¸ç›²ç›®å¡«å……ä¼—æ•°ï¼Œè€Œæ˜¯æ ¹æ®**ç‰©ç†å«ä¹‰**åˆ†å±‚å¤„ç†ã€‚

![Missing Values](images/missing_values.png)

```python
# åˆå¹¶æ•°æ®ä»¥ä¾¿ç»Ÿä¸€åˆ†æ
ntrain = train_df.shape[0]
ntest = test_df.shape[0]
y_train = train_df.SalePrice.values
all_data = pd.concat((train_df.drop(["SalePrice"], axis=1), test_df)).reset_index(drop=True)

# ----------------------------------------------------
# ç­–ç•¥ A: ç‰©ç†ä¸Šä¸å­˜åœ¨ (å¡« "None")
# ä¾‹å¦‚ï¼šæ²¡æœ‰æ³³æ± (PoolQC)ã€æ²¡æœ‰åœ°ä¸‹å®¤(Bsmt...)
cols_fill_none = ["PoolQC", "MiscFeature", "Alley", "Fence", "FireplaceQu",
                  "GarageType", "GarageFinish", "GarageQual", "GarageCond",
                  "BsmtQual", "BsmtCond", "BsmtExposure", "BsmtFinType1", "BsmtFinType2",
                  "MasVnrType"]
for col in cols_fill_none:
    all_data[col] = all_data[col].fillna("None")

# ç­–ç•¥ B: æ•°å€¼ä¸Šä¸å­˜åœ¨ (å¡« 0)
# ä¾‹å¦‚ï¼šæ²¡æœ‰è½¦åº“ï¼Œé¢ç§¯(GarageArea)è‡ªç„¶æ˜¯0
cols_fill_zero = ["GarageYrBlt", "GarageArea", "GarageCars",
                  "BsmtFinSF1", "BsmtFinSF2", "BsmtUnfSF","TotalBsmtSF",
                  "BsmtFullBath", "BsmtHalfBath", "MasVnrArea"]
for col in cols_fill_zero:
    all_data[col] = all_data[col].fillna(0)

# ç­–ç•¥ C: é‚»å±…å¡«å…… (LotFrontage)
# è¡—é“è¿æ¥è·ç¦»é€šå¸¸å’ŒåŒä¸€ä¸ªè¡—åŒºçš„é‚»å±…ç›¸ä¼¼
all_data["LotFrontage"] = all_data.groupby("Neighborhood")["LotFrontage"].transform(
    lambda x: x.fillna(x.median()))

# ç­–ç•¥ D: ä¼—æ•°å¡«å…… (çœŸæ­£çš„é—æ¼æ•°æ®)
cols_mode = ["MSZoning", "Electrical", "KitchenQual", "Exterior1st", "Exterior2nd", "SaleType", "Functional"]
for col in cols_mode:
    all_data[col] = all_data[col].fillna(all_data[col].mode()[0])

# è¡¥ä¸ï¼šUtilities åªæœ‰2ä¸ªç¼ºå¤±
all_data['Utilities'] = all_data['Utilities'].fillna(all_data['Utilities'].mode()[0])

print(f"âœ… æ•°æ®æ¸…æ´—å®Œæˆï¼å‰©ä½™ç¼ºå¤±å€¼: {all_data.isnull().sum().sum()}")
```

---

## Step 6: ç‰¹å¾ç›¸å…³æ€§åˆ†æ <a name="step-6"></a>

é€šè¿‡çƒ­åŠ›å›¾æŸ¥çœ‹å“ªäº›æ•°å€¼å‹ç‰¹å¾ä¸æˆ¿ä»·æœ€ç›¸å…³ã€‚

![Correlation Heatmap](images/heatmap.png)

```python
# åªåˆ†ææ•°å­—å‹ç‰¹å¾
corrmat = train_df.corr(numeric_only=True)
k = 10 
cols = corrmat.nlargest(k, 'SalePrice')['SalePrice'].index
cm = np.corrcoef(train_df[cols].values.T)

# ç»˜å›¾ä»£ç ç•¥ï¼ˆè§æºç ï¼‰
print(f"Top 10 ç›¸å…³ç‰¹å¾: {list(cols)}")
```

---

## Step 7-9: ç¼–ç ã€è®­ç»ƒä¸æäº¤ <a name="step-7"></a>

ä½¿ç”¨ One-Hot ç¼–ç å¤„ç†åˆ†ç±»ç‰¹å¾ï¼Œå¹¶ä½¿ç”¨ç»è¿‡å‚æ•°è°ƒä¼˜çš„ **XGBoost** è¿›è¡Œè®­ç»ƒã€‚

```python
# --- Step 7: ç‹¬çƒ­ç¼–ç  ---
all_data = pd.get_dummies(all_data)
print(f"ç¼–ç åç‰¹å¾æ€»æ•°: {all_data.shape[1]}")

# é‡æ–°æ‹†åˆ†
X_train = all_data[:ntrain]
X_test = all_data[ntrain:]

# --- Step 8: XGBoost è®­ç»ƒ ---
# å‚æ•°å·²é’ˆå¯¹æ•°æ®é›†å¾®è°ƒ
model_xgb = xgb.XGBRegressor(
    colsample_bytree=0.46, gamma=0.04, 
    learning_rate=0.05, max_depth=3, 
    min_child_weight=1.5, n_estimators=2200,
    reg_alpha=0.46, reg_lambda=0.85,
    subsample=0.52, random_state=7, n_jobs=-1
)

print("ğŸš€ å¼€å§‹è®­ç»ƒ XGBoost...")
model_xgb.fit(X_train, y_train)
print("ğŸ‰ è®­ç»ƒå®Œæˆï¼")

# --- Step 9: ç”Ÿæˆæäº¤ ---
log_predictions = model_xgb.predict(X_test)
final_predictions = np.expm1(log_predictions) # åˆ«å¿˜äº†è¿˜åŸ log

submission = pd.DataFrame()
submission['Id'] = test_ID
submission['SalePrice'] = final_predictions
submission.to_csv('submission_eda_xgboost.csv', index=False)
print("âœ… æ–‡ä»¶å·²ç”Ÿæˆï¼šsubmission_eda_xgboost.csv")
```
