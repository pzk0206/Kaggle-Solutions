{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.12.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":17777,"databundleVersionId":869809,"sourceType":"competition"}],"dockerImageVersionId":31234,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2026-01-08T11:25:50.257179Z","iopub.execute_input":"2026-01-08T11:25:50.257424Z","iopub.status.idle":"2026-01-08T11:25:51.321991Z","shell.execute_reply.started":"2026-01-08T11:25:50.257390Z","shell.execute_reply":"2026-01-08T11:25:51.321202Z"}},"outputs":[{"name":"stdout","text":"/kaggle/input/nlp-getting-started/sample_submission.csv\n/kaggle/input/nlp-getting-started/train.csv\n/kaggle/input/nlp-getting-started/test.csv\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nfrom transformers import AutoTokenizer\n\n# 1. è¯»å–æ•°æ®\ntrain_df = pd.read_csv(\"/kaggle/input/nlp-getting-started/train.csv\")\ntest_df = pd.read_csv(\"/kaggle/input/nlp-getting-started/test.csv\")\n\n# 2. è§‚å¯Ÿæ•°æ®\nprint(f\"è®­ç»ƒé›†å¤§å°: {len(train_df)}\")\nprint(f\"æµ‹è¯•é›†å¤§å°: {len(test_df)}\")\nprint(\"\\nå‰5æ¡æ•°æ®ï¼š\")\ndisplay(train_df.head())\n\n# 3. ç®€å•çš„è¡¥å…¨ç­–ç•¥\n# å°†ç¼ºå¤±å€¼å¡«è¡¥ä¸º \"None\" æ˜¯å®Œå…¨æ²¡é—®é¢˜çš„\ntrain_df = train_df.fillna(\"None\")\ntest_df = test_df.fillna(\"None\")\n\n# 4. çœ‹çœ‹æ­£è´Ÿæ ·æœ¬æ¯”ä¾‹\nprint(\"\\nç¾éš¾(1) vs éç¾éš¾(0) çš„æ¯”ä¾‹ï¼š\")\nprint(train_df['target'].value_counts())\n\n# ==========================================\n# ğŸ”¥ å…³é”®è¡¥å……ï¼šåœ¨è¿™é‡Œåˆå§‹åŒ– Tokenizer\n# ==========================================\nmodel_checkpoint = \"distilbert-base-uncased\"\ntokenizer = AutoTokenizer.from_pretrained(model_checkpoint)\n\nprint(\"\\nâœ… å‡†å¤‡å·¥ä½œå®Œæˆï¼\")\nprint(f\"âœ… æ•°æ®å·²åŠ è½½ (train_df)ï¼Œåˆ†è¯å™¨å·²å°±ä½ (tokenizer: {model_checkpoint})\")\nprint(\"ğŸ‘‰ ç°åœ¨ä½ å¯ä»¥æ”¾å¿ƒåœ°è¿è¡Œä¸‹ä¸€ä¸ªæ ¼å­ (Datasetç±») çš„ä»£ç äº†ï¼Œç»å¯¹ä¸ä¼šæŠ¥é”™ï¼\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-08T11:25:51.323202Z","iopub.execute_input":"2026-01-08T11:25:51.323649Z","iopub.status.idle":"2026-01-08T11:26:03.333301Z","shell.execute_reply.started":"2026-01-08T11:25:51.323615Z","shell.execute_reply":"2026-01-08T11:26:03.332596Z"}},"outputs":[{"name":"stdout","text":"è®­ç»ƒé›†å¤§å°: 7613\næµ‹è¯•é›†å¤§å°: 3263\n\nå‰5æ¡æ•°æ®ï¼š\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"   id keyword location                                               text  \\\n0   1     NaN      NaN  Our Deeds are the Reason of this #earthquake M...   \n1   4     NaN      NaN             Forest fire near La Ronge Sask. Canada   \n2   5     NaN      NaN  All residents asked to 'shelter in place' are ...   \n3   6     NaN      NaN  13,000 people receive #wildfires evacuation or...   \n4   7     NaN      NaN  Just got sent this photo from Ruby #Alaska as ...   \n\n   target  \n0       1  \n1       1  \n2       1  \n3       1  \n4       1  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>id</th>\n      <th>keyword</th>\n      <th>location</th>\n      <th>text</th>\n      <th>target</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>1</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>Our Deeds are the Reason of this #earthquake M...</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>4</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>Forest fire near La Ronge Sask. Canada</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>5</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>All residents asked to 'shelter in place' are ...</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>6</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>13,000 people receive #wildfires evacuation or...</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>7</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>Just got sent this photo from Ruby #Alaska as ...</td>\n      <td>1</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}},{"name":"stdout","text":"\nç¾éš¾(1) vs éç¾éš¾(0) çš„æ¯”ä¾‹ï¼š\ntarget\n0    4342\n1    3271\nName: count, dtype: int64\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/48.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"2befd7789a9b490b9220ccb168909bdf"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/483 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"16945ae580de4a66abd7fc55af9561d4"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"8114bcdb197044e0a75c0a992a9beaf5"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"69b03682dde143a5917152b5e52f895f"}},"metadata":{}},{"name":"stdout","text":"\nâœ… å‡†å¤‡å·¥ä½œå®Œæˆï¼\nâœ… æ•°æ®å·²åŠ è½½ (train_df)ï¼Œåˆ†è¯å™¨å·²å°±ä½ (tokenizer: distilbert-base-uncased)\nğŸ‘‰ ç°åœ¨ä½ å¯ä»¥æ”¾å¿ƒåœ°è¿è¡Œä¸‹ä¸€ä¸ªæ ¼å­ (Datasetç±») çš„ä»£ç äº†ï¼Œç»å¯¹ä¸ä¼šæŠ¥é”™ï¼\n","output_type":"stream"}],"execution_count":2},{"cell_type":"code","source":"import torch\nfrom torch.utils.data import Dataset\n\n# å®šä¹‰æˆ‘ä»¬çš„æ•°æ®é›†ç±»\nclass DisasterDataset(Dataset):\n    def __init__(self, df, tokenizer, max_len=128, is_test=False):\n        self.texts = df.text.values  # æ‹¿åˆ°æ‰€æœ‰çš„æ–‡æœ¬\n        self.tokenizer = tokenizer   # æ‹¿åˆ°ä½ çš„ç¿»è¯‘å®˜\n        self.max_len = max_len       # è®¾å®šæ¯å¥è¯æœ€é•¿å¤šå°‘ä¸ªè¯\n        self.is_test = is_test       # æ ‡è®°æ˜¯ä¸æ˜¯æµ‹è¯•é›†\n        \n        # å¦‚æœä¸æ˜¯æµ‹è¯•é›†ï¼Œæˆ‘ä»¬è¦æŠŠæ ‡ç­¾ (0æˆ–1) ä¹Ÿæ‹¿å‡ºæ¥\n        if not is_test:\n            self.targets = df.target.values\n\n    # å‘Šè¯‰æ¨¡å‹æˆ‘ä»¬æ€»å…±æœ‰å¤šå°‘æ¡æ•°æ®\n    def __len__(self):\n        return len(self.texts)\n\n    # æ ¸å¿ƒï¼šè¿™æ˜¯æ¨¡å‹æ¯æ¬¡æ‹¿æ•°æ®æ—¶ä¼šè°ƒç”¨çš„å‡½æ•°\n    def __getitem__(self, item):\n        text = str(self.texts[item])\n        \n        # è®©ç¿»è¯‘å®˜å¹²æ´»ï¼šæŠŠæ–‡æœ¬å˜æˆæ•°å­—\n        encoding = self.tokenizer.encode_plus(\n            text,\n            add_special_tokens=True,  # åŠ ä¸Š [CLS] å’Œ [SEP]\n            max_length=self.max_len,\n            return_token_type_ids=False,\n            padding='max_length',     # ä¸å¤Ÿé•¿çš„è¡¥ 0\n            truncation=True,          # å¤ªé•¿çš„åˆ‡æ‰\n            return_attention_mask=True,\n            return_tensors='pt',      # è¿”å› PyTorch çš„å¼ é‡\n        )\n\n        output = {\n            'input_ids': encoding['input_ids'].flatten(),\n            'attention_mask': encoding['attention_mask'].flatten()\n        }\n        \n        # å¦‚æœæœ‰æ ‡ç­¾ï¼Œä¹Ÿä¸€èµ·è¿”å›\n        if not self.is_test:\n            output['labels'] = torch.tensor(self.targets[item], dtype=torch.long)\n            \n        return output\n\n# æŠŠåˆšæ‰è¯»å–çš„ pandas è¡¨æ ¼è½¬æ¢æˆè¿™ä¸ª Dataset å¯¹è±¡\n# æ³¨æ„ï¼štrain_df å’Œ val_split æ˜¯æˆ‘ä»¬åœ¨ç¬¬ä¸€æ­¥é‡Œåˆ‡åˆ†å¥½çš„\n# ç¡®ä¿ä½ ä¹‹å‰çš„ tokenizer å˜é‡è¿˜åœ¨å†…å­˜é‡Œ\nfull_train_dataset = DisasterDataset(train_df, tokenizer) # å…¨é‡æ•°æ®ï¼ˆå¯é€‰ï¼‰\ntrain_dataset = DisasterDataset(train_df, tokenizer)      # è¿™é‡Œç®€å•èµ·è§ç›´æ¥ç”¨ train_dfï¼Œè¿›é˜¶å¯ä»¥ç”¨åˆ‡åˆ†åçš„ train_split\ntest_dataset = DisasterDataset(test_df, tokenizer, is_test=True)\n\nprint(\"âœ… æ•°æ®é›†ç»„è£…å®Œæˆï¼\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-08T11:26:03.334758Z","iopub.execute_input":"2026-01-08T11:26:03.335009Z","iopub.status.idle":"2026-01-08T11:26:03.343612Z","shell.execute_reply.started":"2026-01-08T11:26:03.334986Z","shell.execute_reply":"2026-01-08T11:26:03.342649Z"}},"outputs":[{"name":"stdout","text":"âœ… æ•°æ®é›†ç»„è£…å®Œæˆï¼\n","output_type":"stream"}],"execution_count":3},{"cell_type":"code","source":"from transformers import AutoModelForSequenceClassification, TrainingArguments, Trainer\nfrom sklearn.metrics import accuracy_score, f1_score\n\n# 1. åŠ è½½é¢„è®­ç»ƒæ¨¡å‹\nmodel = AutoModelForSequenceClassification.from_pretrained(\"distilbert-base-uncased\", num_labels=2)\n\n# 2. å®šä¹‰â€œè¯„åˆ†æ ‡å‡†â€\ndef compute_metrics(pred):\n    labels = pred.label_ids\n    preds = pred.predictions.argmax(-1)\n    acc = accuracy_score(labels, preds)\n    f1 = f1_score(labels, preds, average='weighted')\n    return {'accuracy': acc, 'f1': f1}\n\n# 3. è®¾å®šè®­ç»ƒå‚æ•° (Training Arguments)\ntraining_args = TrainingArguments(\n    output_dir='./results',\n    num_train_epochs=2,\n    per_device_train_batch_size=16,\n    per_device_eval_batch_size=16,\n    learning_rate=2e-5,\n    weight_decay=0.01,\n    \n    # ğŸ”¥ğŸ”¥ğŸ”¥ æ³¨æ„è¿™é‡Œï¼šæˆ‘æ”¹æˆäº†æœ€æ–°çš„ eval_strategy ğŸ”¥ğŸ”¥ğŸ”¥\n    eval_strategy=\"epoch\",  \n    \n    save_strategy=\"epoch\",\n    report_to=\"none\"\n)\n\n# 4. åˆå§‹åŒ–è®­ç»ƒå™¨\ntrainer = Trainer(\n    model=model,\n    args=training_args,\n    train_dataset=train_dataset,\n    eval_dataset=train_dataset,\n    tokenizer=tokenizer,\n    compute_metrics=compute_metrics\n)\n\n# 5. å¼€å§‹è®­ç»ƒï¼\nprint(\"ğŸ”¥ æ¨¡å‹å¼€å§‹è®­ç»ƒ...\")\ntrainer.train()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-08T11:26:03.344552Z","iopub.execute_input":"2026-01-08T11:26:03.344777Z","iopub.status.idle":"2026-01-08T11:28:55.635298Z","shell.execute_reply.started":"2026-01-08T11:26:03.344755Z","shell.execute_reply":"2026-01-08T11:28:55.634544Z"}},"outputs":[{"name":"stderr","text":"2026-01-08 11:26:05.991646: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nE0000 00:00:1767871566.194291      55 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\nE0000 00:00:1767871566.252115      55 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\nW0000 00:00:1767871566.740890      55 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\nW0000 00:00:1767871566.740932      55 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\nW0000 00:00:1767871566.740935      55 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\nW0000 00:00:1767871566.740937      55 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/268M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"026cc49c42714691ab528586e2a2ea5e"}},"metadata":{}},{"name":"stderr","text":"Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n/tmp/ipykernel_55/880408638.py:32: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n  trainer = Trainer(\n","output_type":"stream"},{"name":"stdout","text":"ğŸ”¥ æ¨¡å‹å¼€å§‹è®­ç»ƒ...\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.12/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='476' max='476' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [476/476 02:29, Epoch 2/2]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Epoch</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n      <th>Accuracy</th>\n      <th>F1</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>1</td>\n      <td>No log</td>\n      <td>0.327762</td>\n      <td>0.875608</td>\n      <td>0.874188</td>\n    </tr>\n    <tr>\n      <td>2</td>\n      <td>No log</td>\n      <td>0.291429</td>\n      <td>0.890976</td>\n      <td>0.890083</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"name":"stderr","text":"/usr/local/lib/python3.12/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n","output_type":"stream"},{"execution_count":4,"output_type":"execute_result","data":{"text/plain":"TrainOutput(global_step=476, training_loss=0.3860714054909073, metrics={'train_runtime': 151.2903, 'train_samples_per_second': 100.641, 'train_steps_per_second': 3.146, 'total_flos': 504237152984064.0, 'train_loss': 0.3860714054909073, 'epoch': 2.0})"},"metadata":{}}],"execution_count":4},{"cell_type":"code","source":"import pandas as pd\n\nprint(\"ğŸš€ æ­£åœ¨è®©æ¨¡å‹é¢„æµ‹æµ‹è¯•é›†...\")\n\n# 1. é¢„æµ‹ (ä½¿ç”¨ä½ ä¹‹å‰å®šä¹‰å¥½çš„ trainer å’Œ test_dataset)\npredictions = trainer.predict(test_dataset)\n\n# 2. è½¬æ¢ç»“æœ (æŠŠæ¦‚ç‡å˜æˆ 0 æˆ– 1)\npreds = predictions.predictions.argmax(-1)\n\n# 3. åˆ¶ä½œè¡¨æ ¼\nsubmission = pd.DataFrame()\nsubmission['id'] = test_df['id']  # ç¡®ä¿ test_df è¿˜åœ¨å†…å­˜é‡Œ\nsubmission['target'] = preds\n\n# 4. ä¿å­˜æ–‡ä»¶\n# æ³¨æ„ï¼šè¿™è¡Œä»£ç ä¼šæŠŠæ–‡ä»¶ä¿å­˜åœ¨æœ€å¤–å±‚ï¼Œæ–¹ä¾¿ä½ æ‰¾åˆ°\nsubmission.to_csv('submission.csv', index=False)\n\nprint(\"ğŸ‰ submission.csv ç”ŸæˆæˆåŠŸï¼\")\nprint(\"ğŸ‘‡ è¯·çœ‹å³ä¾§ Output åŒºåŸŸï¼Œç‚¹å‡»åˆ·æ–°å›¾æ ‡ (ğŸ”„) ä¸‹è½½ï¼\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-08T11:28:55.636418Z","iopub.execute_input":"2026-01-08T11:28:55.637217Z","iopub.status.idle":"2026-01-08T11:29:03.937468Z","shell.execute_reply.started":"2026-01-08T11:28:55.637190Z","shell.execute_reply":"2026-01-08T11:29:03.936880Z"}},"outputs":[{"name":"stdout","text":"ğŸš€ æ­£åœ¨è®©æ¨¡å‹é¢„æµ‹æµ‹è¯•é›†...\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":""},"metadata":{}},{"name":"stdout","text":"ğŸ‰ submission.csv ç”ŸæˆæˆåŠŸï¼\nğŸ‘‡ è¯·çœ‹å³ä¾§ Output åŒºåŸŸï¼Œç‚¹å‡»åˆ·æ–°å›¾æ ‡ (ğŸ”„) ä¸‹è½½ï¼\n","output_type":"stream"}],"execution_count":5}]}