# Disaster Tweets Classification: Reaching Top 15% with DistilBERT ğŸš€

> **Kaggle Competition:** [Natural Language Processing with Disaster Tweets](https://www.kaggle.com/c/nlp-getting-started)
> **Score:** 0.83450 (Top 15%)
> **Model:** DistilBERT (Hugging Face Transformers)

## 1. Project Overview (é¡¹ç›®ç®€ä»‹)
åœ¨è¿™ä¸ªé¡¹ç›®ä¸­ï¼Œæˆ‘å‚åŠ äº† Kaggle çš„ç»å…¸çš„ NLP å…¥é—¨ç«èµ›ï¼š**Real Disaster Tweets**ã€‚ä»»åŠ¡æ˜¯æ„å»ºä¸€ä¸ªæœºå™¨å­¦ä¹ æ¨¡å‹ï¼Œåˆ¤æ–­ä¸€æ¡ Twitter æ¨æ–‡æ˜¯å¦åœ¨æè¿°çœŸå®çš„ç¾éš¾ï¼ˆBinary Classificationï¼‰ã€‚

* **éš¾ç‚¹ï¼š** æ¨æ–‡æ˜¯éæ­£å¼æ–‡æœ¬ï¼ŒåŒ…å«å¤§é‡çš„æ‹¼å†™é”™è¯¯ã€ç¼©å†™ã€Emoji å’Œ URLï¼Œä¸”æ•°æ®é›†ä¸­å­˜åœ¨æ ‡ç­¾å™ªå£°ã€‚
* **æˆ‘çš„æ–¹æ¡ˆï¼š** ä½¿ç”¨é¢„è®­ç»ƒçš„ **DistilBERT** æ¨¡å‹è¿›è¡Œå¾®è°ƒ (Fine-tuning)ï¼Œç›¸æ¯”ä¼ ç»Ÿ LSTM/RNN æ–¹æ³•ï¼Œèƒ½æ›´å¥½åœ°ç†è§£ä¸Šä¸‹æ–‡è¯­ä¹‰ã€‚

## 2. Tech Stack (æŠ€æœ¯æ ˆ)
* **Python 3.10**
* **PyTorch** (Deep Learning Framework)
* **Hugging Face Transformers** (Pre-trained Models)
* **Pandas & Scikit-Learn** (Data Analysis)
* **Kaggle GPU (T4 x2)** (Hardware Accelerator)

## 3. My Approach (æ ¸å¿ƒæ€è·¯)

### 3.1 Data Preprocessing
* ä½¿ç”¨ `distilbert-base-uncased` çš„ Tokenizer è¿›è¡Œåˆ†è¯ã€‚
* å¤„ç†ç¼ºå¤±å€¼ï¼šå°† text å­—æ®µä¸­çš„ `NaN` å¡«å……ä¸º "None"ã€‚
* è®¾å®š `max_length=128` ä»¥è¦†ç›–ç»å¤§å¤šæ•°æ¨æ–‡é•¿åº¦ã€‚

### 3.2 Model Training
æˆ‘é€‰æ‹©äº† **DistilBERT**ï¼Œå› ä¸ºå®ƒåœ¨ä¿æŒ BERT 97% æ€§èƒ½çš„åŒæ—¶ï¼Œå‚æ•°é‡å‡å°‘äº† 40%ï¼Œè®­ç»ƒé€Ÿåº¦æå‡äº† 60%ã€‚

**Hyperparameters:**
* `batch_size`: 16
* `learning_rate`: 2e-5
* `epochs`: 2 (To prevent overfitting)
* `optimizer`: AdamW

```python
# Training Arguments Configuration
training_args = TrainingArguments(
    output_dir='./results',
    num_train_epochs=2,
    per_device_train_batch_size=16,
    eval_strategy="epoch",  # Evaluate every epoch
    save_strategy="epoch",
    learning_rate=2e-5,
    report_to="none"
)
