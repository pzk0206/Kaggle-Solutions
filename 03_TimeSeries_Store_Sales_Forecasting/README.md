# Store Sales Prediction: Industrial Time Series Forecasting with XGBoost ğŸ“ˆ

> **Kaggle ç«èµ›:** [Store Sales - Time Series Forecasting](https://www.kaggle.com/competitions/store-sales-time-series-forecasting)
> **å…¬å¼€æ¦œå¾—åˆ†:** 0.46139 (Top 20%) ğŸš€
> **æ ¸å¿ƒæ¨¡å‹:** XGBoost Regressor (GPU Accelerated)
> **å…³é”®ç­–ç•¥:** Lag Features (æ»åç‰¹å¾) + Rolling Windows (æ»‘åŠ¨çª—å£) + Time-Based Split

## 1. Project Overview (é¡¹ç›®ç®€ä»‹)
æœ¬é¡¹ç›®åŸºäº Kaggle ç»å…¸çš„æ—¶é—´åºåˆ—ç«èµ›ã€‚ä»»åŠ¡æ˜¯é¢„æµ‹å„ç“œå¤šå°”å¤§å‹é›¶å”®å•† CorporaciÃ³n Favorita æ——ä¸‹ **54 å®¶å•†åº—**ã€**33 ç±»å•†å“**åœ¨æœªæ¥ **16 å¤©**çš„æ—¥é”€é‡ã€‚

* **éš¾ç‚¹ (Challenges)ï¼š**
    * **å¤šå˜é‡å¹²æ‰°ï¼š** é”€é‡å—æ²¹ä»·æ³¢åŠ¨ï¼ˆå®è§‚ç»æµï¼‰ã€èŠ‚å‡æ—¥ï¼ˆå±€éƒ¨äº‹ä»¶ï¼‰ã€å‘è–ªæ—¥ç­‰å¤šé‡å› ç´ å½±å“ã€‚
    * **æ•°æ®é‡å¤§ï¼š** è®­ç»ƒé›†åŒ…å«è¶…è¿‡ 300 ä¸‡è¡Œæ•°æ®ã€‚
    * **æœªæ¥æ³„éœ²é£é™©ï¼š** æµ‹è¯•é›†è¦æ±‚é¢„æµ‹æœªæ¥ 16 å¤©ï¼Œå¿…é¡»é˜²æ­¢åœ¨ç‰¹å¾å·¥ç¨‹ä¸­â€œçœ‹è§æœªæ¥â€ã€‚
* **æˆ‘çš„æ–¹æ¡ˆ (My Approach)ï¼š**
    * **ç›®æ ‡å˜æ¢ï¼š** ä½¿ç”¨ **Log1p** å¤„ç†é•¿å°¾åˆ†å¸ƒçš„é”€é‡æ•°æ®ã€‚
    * **ç¯å¢ƒæ„ŸçŸ¥ï¼š** æ„å»ºç²¾å‡†çš„**å‡æœŸåŒ¹é…é€»è¾‘**ï¼ˆåŸå¸‚å¯¹åŸå¸‚ï¼‰å’Œæ²¹ä»·æ’å€¼ã€‚
    * **æ—¶åºé­”æ³•ï¼š** æ”¾å¼ƒç®€å•çš„æ—¥æœŸç‰¹å¾ï¼Œè½¬è€Œæ„å»º **Lag 16+** (æ»åç‰¹å¾) å’Œ **Rolling Mean** (è¶‹åŠ¿ç‰¹å¾)ï¼Œè¿™æ˜¯æåˆ†çš„å…³é”®ã€‚

## 2. Tech Stack (æŠ€æœ¯æ ˆ)
* **Python 3.8+**
* **Pandas & NumPy** (High-performance Data Manipulation)
* **XGBoost** (Gradient Boosting with `tree_method='hist'`)
* **Scikit-Learn** (Label Encoding, Metrics)
* **Matplotlib** (Visualization)

---

## 3. Implementation Details (æ ¸å¿ƒå®ç°)

### 3.1 Data Preprocessing & Context Engineering (æ•°æ®é¢„å¤„ç†ä¸ç¯å¢ƒæ„ŸçŸ¥)

ä¸ºäº†æ„å»ºé«˜è´¨é‡çš„è®­ç»ƒæ•°æ®ï¼Œæˆ‘æ‰§è¡Œäº†ä¸‰ä¸ªå…³é”®æ­¥éª¤ï¼š
1.  **ç›®æ ‡å˜æ¢ï¼š** å¯¹é•¿å°¾åˆ†å¸ƒçš„ `sales` è¿›è¡Œ **Log1p** å˜æ¢ï¼Œä½¿å…¶ç¬¦åˆ RMSLE è¯„ä¼°æŒ‡æ ‡ã€‚
2.  **ç¯å¢ƒæ„ŸçŸ¥ (Context)ï¼š** ç¼–å†™**ç²¾å‡†å‡æœŸåŒ¹é…é€»è¾‘**ã€‚å•çº¯çš„ Merge ä¼šå¼•å…¥å™ªéŸ³ï¼ˆä¾‹å¦‚â€œåŸºå¤šâ€çš„å•†åº—ä¸åº”å—â€œæ˜†å¡â€åœ°æ–¹å‡æœŸçš„å½±å“ï¼‰ï¼Œåªæœ‰å½“ `Store City == Holiday Locale` æ—¶æ‰æ ‡è®°ä¸ºå‡æœŸã€‚
3.  **æ—¶é—´åˆ‡åˆ† (Split)ï¼š** ä¸¥ç¦éšæœºåˆ‡åˆ†ï¼Œä¸¥æ ¼æŒ‰ç…§æ—¶é—´è½´åˆ’åˆ†è®­ç»ƒé›† (`2013-2016`) å’ŒéªŒè¯é›† (`2017`)ã€‚

![Target Distribution](images/target_dist.png)

```python
import pandas as pd
import numpy as np
import xgboost as xgb

# 1. ç›®æ ‡å€¼ Log å¹³æ»‘ (Target Log Transformation)
train['sales'] = np.log1p(train['sales'])

# 2. å‡æœŸç‰¹å¾ç²¾å‡†åŒ¹é… (Precise Holiday Matching)
# ç­–ç•¥ï¼šåªæœ‰å½“ å•†åº—æ‰€åœ¨åŸå¸‚ == å‡æœŸåº†ç¥åŸå¸‚ æ—¶ï¼Œæ‰æ ‡è®°ä¸ºå‡æœŸ
def apply_local_holidays(df, local_hols, merge_col):
    merged = df.merge(local_hols[['date', 'locale_name']], 
                      left_on=['date', merge_col], 
                      right_on=['date', 'locale_name'], 
                      how='left')
    is_local_hol = merged['locale_name'].notna()
    # è¿™æ˜¯ä¸€ä¸ªç´¯åŠ è¿‡ç¨‹ï¼Œä¿ç•™å·²æœ‰çš„å‡æœŸæ ‡è®°
    return np.maximum(df.get('is_holiday', 0), is_local_hol.astype(int))

# åˆå§‹åŒ–å¹¶åº”ç”¨é€»è¾‘
train['is_holiday'] = 0
train['is_holiday'] = apply_local_holidays(train, local_holidays, 'city')
train['is_holiday'] = apply_local_holidays(train, regional_holidays, 'state')

print("âœ… å‡æœŸç‰¹å¾æ¸…æ´—å®Œæˆ (Noise Reduction Applied)")

# 3. åŸºäºæ—¶é—´çš„ä¸¥æ ¼åˆ‡åˆ† (Time-Based Split)
# è®­ç»ƒé›†: 2013 ~ 2016 | éªŒè¯é›†: 2017-01-01 ~ 2017-08-15
train_mask = train['date'] < '2017-01-01'
val_mask = train['date'] >= '2017-01-01'

X_train = train.loc[train_mask, features]
y_train = train.loc[train_mask, 'sales']
X_val = train.loc[val_mask, features]
y_val = train.loc[val_mask, 'sales']

print(f"âœ… æ•°æ®å‡†å¤‡å®Œæˆã€‚è®­ç»ƒé›†æ ·æœ¬æ•°: {X_train.shape[0]}")

### 3.2 Temporal Feature Engineering (æ—¶åºç‰¹å¾æŒ–æ˜)
è¿™æ˜¯æœ¬é¡¹ç›®æœ€æ ¸å¿ƒçš„æåˆ†ç‚¹ã€‚ä¸ºäº†é¢„æµ‹æœªæ¥ 16 å¤©ï¼Œæˆ‘ä»¬ä¸èƒ½ä½¿ç”¨â€œæ˜¨å¤©â€çš„æ•°æ®ï¼ˆLag 1ï¼‰ï¼Œå› ä¸ºåœ¨é¢„æµ‹ç¬¬ 2 å¤©æ—¶æ•°æ®ä¼šç¼ºå¤±ã€‚

* **Lag 16 Strategy:** å¼ºåˆ¶æ¨¡å‹å›çœ‹ 16 å¤©å‰çš„æ•°æ®ï¼Œç¡®ä¿æ¨ç†é˜¶æ®µæ•°æ®å®Œæ•´ã€‚
* **Rolling Mean:** ä½¿ç”¨æ»‘åŠ¨çª—å£å¹³æ»‘å•æ—¥é”€é‡çš„éšæœºæ³¢åŠ¨ï¼ˆå¦‚çªå‘å¤©æ°”å½±å“ï¼‰ã€‚

![Feature Engineering](images/lag_features.png)

```python
# å¿…é¡»åˆå¹¶ Train å’Œ Test è¿›è¡Œæ—¶åºè®¡ç®—ï¼Œå¹¶æŒ‰ Store/Family æ’åº
all_data = pd.concat([train, test], axis=0).sort_values(['store_nbr', 'family', 'date'])

# --- ç­–ç•¥ A: æ»åç‰¹å¾ (The Rear-View Mirror) ---
# ä¸ºä»€ä¹ˆæ˜¯ 16? å› ä¸ºæµ‹è¯•é›†æ—¶é•¿ä¸º 16 å¤©ã€‚
# è¿™æ˜¯é˜²æ­¢æ•°æ®æ³„éœ² (Data Leakage) çš„æœ€å°å®‰å…¨è·ç¦»ã€‚
lags = [16, 30, 60, 90]
for lag in lags:
    all_data[f'lag_{lag}'] = all_data.groupby(['store_nbr', 'family'])['sales'].shift(lag)

# --- ç­–ç•¥ B: æ»‘åŠ¨å¹³å‡ (Trend Capturing) ---
# è®¡ç®— 16 å¤©å‰çš„è¿‡å» 30 å¤©å¹³å‡é”€é‡
all_data['rolling_mean_30'] = all_data.groupby(['store_nbr', 'family'])['sales'] \
    .transform(lambda x: x.shift(16).rolling(30).mean())

print("âœ… é«˜é˜¶æ—¶åºç‰¹å¾æ„å»ºå®Œæˆ (Lags + Rolling Means)")
